# AI-Driven Live Portrait Motion Animator

A real-time AI-powered portrait animation system that transforms static images into animated characters driven by webcam input, optimized for streaming overlays via Spout/NDI.

## ğŸ¯ Features

- **AI-Powered Animation**: Uses Live Portrait or similar models for high-quality real-time image animation
- **GPU Accelerated**: Optimized for NVIDIA RTX 3080 (60-70 FPS target at 1080p)
- **Real-Time Tracking**: MediaPipe face landmark detection for responsive motion capture
- **Streaming Ready**: Spout and NDI output support for OBS/streaming software
- **Runtime Image Switching**: Change character images without restarting
- **Extensible Architecture**: Modular design supports multiple AI animation models
- **Performance Monitoring**: Built-in benchmarking and FPS metrics

## ğŸ“‹ System Requirements

### Minimum Requirements
- **GPU**: NVIDIA RTX 2060 or better (with 6GB+ VRAM)
- **CPU**: Intel i5-8400 / AMD Ryzen 5 2600 or better
- **RAM**: 16GB
- **OS**: Windows 10/11 (for Spout), Linux (for development)
- **Python**: 3.8 or higher

### Recommended for 60+ FPS (RTX 3080)
- **GPU**: NVIDIA RTX 3080 (10GB VRAM)
- **CPU**: Intel i7-9700K / AMD Ryzen 7 3700X or better
- **RAM**: 32GB
- **Storage**: SSD for model loading

## ğŸš€ Quick Start

### 1. Installation

```bash
# Clone the repository
git clone https://github.com/GilbertoBitt/stream-motion-animator.git
cd stream-motion-animator

# Create virtual environment
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt
```

### 2. Download AI Models

```bash
# Download Live Portrait model (example)
python scripts/download_models.py --model live-portrait

# Or manually download and place in models/ directory
```

### 3. Run the Animator

```bash
# Basic usage with webcam
python main.py --image path/to/your/portrait.jpg

# With specific settings for RTX 3080
python main.py --image portrait.jpg --gpu-batch-size 4 --target-fps 60

# Enable NDI output
python main.py --image portrait.jpg --output ndi --ndi-name "AI_Avatar"

# Enable Spout output (Windows only)
python main.py --image portrait.jpg --output spout --spout-name "AI_Avatar"
```

### 4. Benchmark Your System

```bash
# Run performance test
python scripts/benchmark.py --image test_image.jpg --duration 60

# Test different batch sizes
python scripts/benchmark.py --auto-tune
```

## ğŸ—ï¸ Architecture

```
stream-motion-animator/
â”œâ”€â”€ main.py                 # Main application entry point
â”œâ”€â”€ config.yaml             # Default configuration
â”œâ”€â”€ requirements.txt        # Python dependencies
â”‚
â”œâ”€â”€ src/                    # Source code
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ capture/           # Video capture module
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ webcam.py
â”‚   â”œâ”€â”€ tracking/          # Face landmark detection
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ mediapipe_tracker.py
â”‚   â”œâ”€â”€ animation/         # AI animation models
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ base_model.py
â”‚   â”‚   â””â”€â”€ live_portrait.py
â”‚   â”œâ”€â”€ output/            # Output management (Spout/NDI)
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ base_output.py
â”‚   â”‚   â”œâ”€â”€ spout_output.py
â”‚   â”‚   â””â”€â”€ ndi_output.py
â”‚   â”œâ”€â”€ pipeline/          # Main processing pipeline
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ animator_pipeline.py
â”‚   â””â”€â”€ utils/             # Utilities
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ config.py
â”‚       â”œâ”€â”€ logger.py
â”‚       â”œâ”€â”€ metrics.py
â”‚       â””â”€â”€ image_utils.py
â”‚
â”œâ”€â”€ models/                # AI model weights (gitignored)
â”‚   â””â”€â”€ .gitkeep
â”‚
â”œâ”€â”€ scripts/               # Utility scripts
â”‚   â”œâ”€â”€ download_models.py
â”‚   â”œâ”€â”€ benchmark.py
â”‚   â””â”€â”€ convert_image.py
â”‚
â”œâ”€â”€ docs/                  # Documentation
â”‚   â”œâ”€â”€ SETUP.md
â”‚   â”œâ”€â”€ TROUBLESHOOTING.md
â”‚   â”œâ”€â”€ GPU_OPTIMIZATION.md
â”‚   â””â”€â”€ MODEL_INTEGRATION.md
â”‚
â”œâ”€â”€ examples/              # Example configurations and images
â”‚   â”œâ”€â”€ config_rtx3080.yaml
â”‚   â””â”€â”€ sample_portrait.jpg
â”‚
â””â”€â”€ tests/                 # Unit tests
    â”œâ”€â”€ __init__.py
    â””â”€â”€ test_pipeline.py
```

## âš™ï¸ Configuration

Edit `config.yaml` or create `config_local.yaml` for custom settings:

```yaml
# Video Input
capture:
  device_id: 0
  width: 1920
  height: 1080
  fps: 60

# Face Tracking
tracking:
  model: mediapipe
  confidence_threshold: 0.5
  smoothing: true

# AI Animation
animation:
  model: live_portrait
  gpu_id: 0
  batch_size: 4          # Adjust based on GPU memory
  inference_precision: fp16  # fp32, fp16, or int8
  use_tensorrt: false    # Advanced optimization

# Output
output:
  enabled: [display]     # Options: display, spout, ndi
  resolution: [1920, 1080]
  fps_target: 60

# Performance
performance:
  max_latency_ms: 20
  frame_skip_threshold: 0.9
  prefetch_frames: 2
```

## ğŸ® Usage Examples

### Runtime Image Switching

```python
# Press keys during runtime:
# '1', '2', '3' - Switch between loaded images
# 'l' - Load new image from file dialog
# 'q' - Quit application
# 's' - Save current frame
# 'b' - Toggle benchmarking display
```

### Command Line Options

```bash
# Full options
python main.py \
  --image portrait.jpg \
  --config config_rtx3080.yaml \
  --gpu-id 0 \
  --batch-size 4 \
  --output spout ndi \
  --target-fps 60 \
  --enable-metrics \
  --log-level INFO
```

## ğŸ“Š Performance Optimization

### For RTX 3080 Users

1. **Optimal Batch Size**: Start with batch_size=4, adjust based on VRAM usage
2. **Precision**: Use fp16 for 2x performance with minimal quality loss
3. **Resolution**: 1080p is optimal; 4K may require lower batch size
4. **Monitoring**: Enable metrics to identify bottlenecks

```bash
# Auto-tune for your system
python scripts/benchmark.py --auto-tune --gpu-id 0
```

### Expected Performance

| GPU | Resolution | Batch Size | FPS | Latency |
|-----|-----------|-----------|-----|---------|
| RTX 3080 | 1080p | 4 | 65-70 | <15ms |
| RTX 3080 | 1080p | 2 | 70-80 | <12ms |
| RTX 3070 | 1080p | 2 | 50-60 | <18ms |
| RTX 2060 | 1080p | 1 | 30-40 | <25ms |

## ğŸ”§ Troubleshooting

### Low FPS / High Latency

1. **Check GPU Usage**: 
   ```bash
   nvidia-smi -l 1
   ```
   
2. **Reduce Batch Size**: Lower `batch_size` in config
3. **Enable FP16**: Set `inference_precision: fp16`
4. **Close Background Apps**: Free up GPU memory
5. **Update Drivers**: Ensure latest NVIDIA drivers

### Common Issues

**Issue**: "CUDA out of memory"
- **Solution**: Reduce batch_size or use lower resolution

**Issue**: "Spout not found"
- **Solution**: Windows only, install SpoutGL library

**Issue**: "NDI library not found"
- **Solution**: Install NDI SDK from NewTek website

**Issue**: "Model file not found"
- **Solution**: Run `python scripts/download_models.py`

See [TROUBLESHOOTING.md](docs/TROUBLESHOOTING.md) for detailed solutions.

## ğŸ”Œ Output Integration

### OBS Studio Setup

#### Spout (Windows)
1. Install OBS-Spout2-Plugin
2. Add "Spout2 Capture" source
3. Select "AI_Avatar" from dropdown

#### NDI
1. Install OBS-NDI plugin
2. Add "NDI Source"
3. Select "AI_Avatar" from network sources

## ğŸ§© Extending with New Models

The system is designed to support multiple AI animation models:

```python
# src/animation/your_model.py
from src.animation.base_model import BaseAnimationModel

class YourModel(BaseAnimationModel):
    def load_model(self):
        # Load your model
        pass
    
    def animate(self, source_image, driving_landmarks):
        # Generate animated frame
        pass
```

See [MODEL_INTEGRATION.md](docs/MODEL_INTEGRATION.md) for details.

## ğŸ¤ Supported Models (Roadmap)

- [x] **Live Portrait** - Primary model, optimized for real-time
- [ ] **AnimateAnyone** - Higher quality, slower
- [ ] **SadTalker** - Audio-driven animation
- [ ] **Custom ONNX Models** - Bring your own model

## ğŸ“ License

MIT License - See LICENSE file for details

## ğŸ™ Acknowledgments

- [Live Portrait](https://github.com/lylalabs/live-portrait) - AI animation model
- [MediaPipe](https://developers.google.com/mediapipe) - Face tracking
- [Spout](https://spout.zeal.co/) - Video sharing framework
- [NDI](https://www.ndi.tv/) - Network Device Interface

## ğŸ“§ Support

- **Issues**: [GitHub Issues](https://github.com/GilbertoBitt/stream-motion-animator/issues)
- **Discussions**: [GitHub Discussions](https://github.com/GilbertoBitt/stream-motion-animator/discussions)

## ğŸ—ºï¸ Roadmap

- [x] Core pipeline implementation
- [x] Live Portrait integration
- [x] GPU optimization
- [ ] Audio-driven animation
- [ ] Multi-character support
- [ ] Web-based configuration UI
- [ ] Pre-built model zoo
- [ ] Cloud rendering support
